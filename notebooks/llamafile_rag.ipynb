{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval-Augmented Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Steps to Download and Run any open LLM model compliant with LlamaFile, usually with .llamafile extension\n",
    "\n",
    "0. you can use [HugingFace](https://huggingface.co/) to download an open llm for local server runtime and embeding capabilites.\n",
    " \n",
    "   here's how to start, you can select a set of different of models available on the site; but since we're looking for llamafile llms lets :\n",
    "   - > filter by models\n",
    "   - > select libraries  \n",
    "   - > check llamafile  \n",
    "   - > download for example `llava-v1.5-7b-q4.llamafile` from `Mozzila` project\n",
    "\n",
    "1. **Download the LLaVA Model**  \n",
    "   Download the file `llava-v1.5-7b-q4.llamafile` (4.29 GB).\n",
    "\n",
    "2. **Move file for better organization**  \n",
    "   Move the `.llamafile` model to `openllms/llamafiles` subfolder\n",
    "\n",
    "3. **Grant Execution Permission (macOS, Linux, BSD)**  \n",
    "   If you're using macOS, Linux, or BSD, you need to allow the file to be executed. Run the following command:  \n",
    "   ```bash\n",
    "   chmod +x llava-v1.5-7b-q4.llamafile\n",
    "\n",
    "4. **Satrt server without browser chatbot UI**\n",
    "    ```bash\n",
    "    ./llava-v1.5-7b-q4.llamafile  --server --nobrowser --embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval-Augmented Generation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Indexing \n",
    "<img src=\"../assets/rag/how-indexing-works.jpg\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieval\n",
    "<img src=\"../assets/rag/rag-system-retrieval.jpg\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quering \n",
    "<img src=\"../assets/rag/how-rag-works-prompt-query.jpg\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RAG overall architecure\n",
    "<img src=\"../assets/rag/rag-all-steps.png\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook debut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import LlamafileEmbeddings\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_chroma import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain_core.documents import Document\n",
    "from langgraph.graph import START, StateGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing_extensions import List, TypedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_path = Path().resolve().parent / \"assets\" / \"data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Point to local LLamafile running localy as a server  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm = Llamafile()\n",
    "llm = ChatOpenAI(\n",
    "    model=\"LLaMA_CPP\", base_url=\"http://localhost:8080/v1\", api_key=\"sk-no-key-required\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instanciate an embedding instance based on the local llamafile llm  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = LlamafileEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instanciate a vectore/embeeding database for vectores indexation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = Chroma(embedding_function=embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embeddings functions for corpus/text into a dense numerical representation of text in a vector space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load the PDF document\n",
    "def load_pdf(file_path: str):\n",
    "    loader = PyPDFLoader(file_path)\n",
    "    documents = loader.load()\n",
    "    return documents\n",
    "\n",
    "\n",
    "# Step 2: Split the document into chunks\n",
    "def split_documents(documents: list[Document]):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "    return text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start creating embedding documents from a pdf through PyPDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_splits = []\n",
    "for file in documents_path.glob(\"*.pdf\"):\n",
    "    documents = load_pdf(str(file))\n",
    "    all_splits.extend(split_documents(documents=documents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Indexing embeddings/vectors into a special dedicated database "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index chunks\n",
    "_ = vector_store.add_documents(documents=all_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieval functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define prompt for question-answering\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "\n",
    "# Define state for application\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "\n",
    "\n",
    "# Define application steps\n",
    "def retrieve(state: State):\n",
    "    retrieved_docs = vector_store.similarity_search(state[\"question\"])\n",
    "    return {\"context\": retrieved_docs}\n",
    "\n",
    "\n",
    "def generate(state: State):\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"answer\": response.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile application and test\n",
    "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### you can now query your pdf documents that were in *`assets/data`* folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zlatan Ibrahimovic is a professional soccer player who was born on October 3, 1981, in Malm√∂, Sweden. He is known for his skills as a striker and has played for several teams, including Ajax, Juventus, Inter Milan, Barcelona, and Paris Saint-Germain. He has also played for the Swedish national team and has won numerous awards, including the Golden Ball. Ibrahimovic is known for his outspoken comments and has referred to himself in the third person. He has scored many memorable goals throughout his career, including a record 12 times for the Swedish player of the year award.</s>\n"
     ]
    }
   ],
   "source": [
    "response = graph.invoke({\"question\": \"What can you tell me about Zlatan Ibrahimovic ?\"})\n",
    "print(response[\"answer\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local-llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
